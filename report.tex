%% Standard start of a latex document
\documentclass[letterpaper,12pt]{article}
%% Always use 12pt - it is much easier to read
%% Things written after '%' sign, are ignored by the latex editor - they are how to introduce comments into your .tex source
%% Anything mathematics related should be put in between '$' signs.

%% Set some names and numbers here so we can use them below
\newcommand{\name}{James Wu}

%%%%%%
%% There is a bit of stuff below which you should not have to change
%%%%%%

%% AMS mathematics packages - they contain many useful fonts and symbols.
\usepackage{amsmath, amsfonts, amssymb, bm}

%% The geometry package changes the margins to use more of the page, I suggest
%% using it because standard latex margins are chosen for articles and letters,
%% not homework.
\usepackage[paper=letterpaper,left=25mm,right=25mm,top=30mm,bottom=30mm]{geometry}
%% For details of how this package work, google the ``latex geometry documentation''.

%% Fancy headers and footers - make the document look nice
\usepackage{fancyhdr} %% for details on how this work, search-engine ``fancyhdr documentation''
\pagestyle{fancy}

\usepackage{graphicx}
\usepackage{adjustbox}

\setlength{\headheight}{15pt}

%% These put horizontal lines between the main text and header and footer.
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
%%%

%%%%%%
%% The above stuff is the same as the first template, but now we are starting to prove things, so we'd like to have a
%% good proof environment that gives us nice formatting and a little square at the end.
%% We'd also like a nice Result environment that prints that up nicely too.
%% Thankfully this exists in latex in the amsthm package
\usepackage{amsthm}
\newtheorem*{thm}{Theorem}
%% This creates a new theorem-like environment called "result", that will be titled "Result".
%% See below for examples of how to use this.
%%%%%%
\usepackage{enumitem}
%% This package allows us to make nice ordered lists with numbers, letters or roman numerals

\usepackage{titlesec}

\usepackage[hang,flushmargin]{footmisc}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\allowdisplaybreaks

\renewcommand{\arraystretch}{1.4}

\usepackage{empheq}

\newcommand*\wfbox[1]{\fbox{\hspace{0.4em}#1\hspace{0.4em}}}
\numberwithin{table}{section}
\numberwithin{figure}{section}
\numberwithin{equation}{section}

%% Useful commands
\renewcommand*{\qed}{\hfill\ensuremath{\square}}

\newcommand*{\uvec}[1]{\hat{\bm{#1}}}

\newcommand*{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand*{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand*{\nderiv}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand*{\npderiv}[3]{\frac{\partial^{#3} #1}{\partial #2^{#3}}}
\newcommand*{\divg}[1]{\nabla \cdot \mathbf{#1}}
\newcommand*{\curl}[1]{\nabla \times \mathbf{#1}}

\newcommand*{\abs}[1]{\left| #1 \right|}
\newcommand*{\norm}[1]{\abs{\abs{\mathbf{#1}}}}

\newcommand*{\ev}[1]{\left<#1\right>}

\renewcommand*{\Re}[1]{\text{Re}\left(#1\right)}
\renewcommand*{\Im}[1]{\text{Im}\left(#1\right)}

\newcommand*{\qimg}[2]{\\ \begin{center}\includegraphics[scale=#1]{#2}\end{center}}

\newcommand*{\Arg}[1]{\text{Arg}\left(#1\right)}
\newcommand*{\Log}[1]{\text{Log}\left(#1\right)}
\newcommand*{\Tr}[1]{\text{Tr}\left(#1\right)}
\newcommand*{\Binom}[2]{\text{Binom}\left(#1, #2\right)}

\newcommand*{\ket}[1]{\left|#1\right>}
\newcommand*{\bra}[1]{\left<#1\right|}
\newcommand*{\braket}[2]{\left<#1\right|\left.\!#2\right>}
\newcommand*{\comm}[2]{\left[#1, #2\right]}

% Quick 2-vector and 2x2 matrix
\newcommand*{\qvec}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\newcommand*{\qmat}[4]{\begin{pmatrix} #1 & #2 \\ #3 & #4 \end{pmatrix}}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{img/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{img/#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}
\newcommand{\ccaption}[1]{\caption{\textit{#1}}}

% References
\newcommand{\reffig}[1]{\textbf{Figure \ref{#1}}}
\newcommand{\reftab}[1]{\textbf{Table \ref{#1}}}

%%

\begin{document}
\begin{flushleft}

    % TODO title page, etc.
    % TODO header, footer formatting

    \section{Preface}

    \subsection{Context}

    \subsection{Acknowledgements}

    \subsection{Contact Me}
    % TODO

    \section{Code Usage}

    \subsection{Code Structure}

    \subsection{Running Scripts}

    % TODO

    \section{Morris-Lecar Neuron}

    This project investigated the Morris-Lecar model of a neuron. This section goes over background theory on the model.
    
    \subsection{Circuit Model}

    % ML intro
    The Morris-Lecar description \cite{ml} models a neuron as having a membrane potential $v$, defined to be the difference in voltage between the inside and outside of the neuron cell. Current flows through potassium and calcium channels in the membrane, labelled as $I_K$ and $I_{Ca}$, respectively. There is also some current arising form other ions, which is collectively totalled as the leak current $I_L$. The combined potassium, calcium, and leak channels each have an effective conductance (or, taking reciprocals, resistance) and potential. Finally, the membrane has a capacitance $C$. \reffig{fig:ml-circuit} depicts a circuit for this model.

    % ML neuron circuit
    \begin{figure}

        \centering
    
        \centerfig{0.6}{ml-circuit.jpg}
        \caption{TODO + cite ML}
    
        \label{fig:ml-circuit}
    
    \end{figure}

    % ML circuit equation
    Consequently, the membrane potential obeys the following first order differential equation:
    \begin{equation}
        C\deriv{v}{t} = I - g_K (v - V_K) - g_{Ca} (v - V_{Ca}) - g_L (v - V_L)
        \label{eqn:ml-circuit}
    \end{equation}

    % ML conductances
    Nominally, the potassium and calcium conductances are non-constant. Rather, they obey the following equations:
    \begin{align}
        g_K &= \bar{g}_K w \\
        g_{Ca} &= \bar{g}_{Ca} m
    \end{align}
    where $\bar{g}_K, \bar{g}_{Ca}$ are constant.
    
    % ML voltage gating variables
    At constant $v$, the parameters $x = w, m$ are governed by first order differential equations of the following form:
    \begin{equation}
        \label{eqn:ml-voltage-gating}
        \deriv{x}{t} = \lambda_x(v) (x_\infty(v) - x)
    \end{equation}
    
    % ML voltage gating vars simplification
    However, the $m$ timescale is much shorter than the $w$ timescale, so that $m \approx m_\infty(v)$ in (\ref{eqn:ml-circuit}). Furthermore, to be consistent with \cite{snm}, we may re-arrange the $x = w$ version of (\ref{eqn:ml-voltage-gating}) to be of the following form:
    \begin{equation}
        \label{eqn:w-dyn}
        \deriv{w}{t} = \alpha(v)(1  - w) - \beta(v)w
    \end{equation}
    where
    \begin{align}
        \label{eqn:alpha-def}
        \alpha(v) &= \frac{1}{2} \phi \cosh{\left(\frac{v - V_3}{2V_4}\right)}\left(1 + \tanh{\left(\frac{v - V_3}{V_4}\right)}\right) \\
        \label{eqn:beta-def}
        \beta(v) &= \frac{1}{2} \phi \cosh{\left(\frac{v - V_3}{2V_4}\right)}\left(1 - \tanh{\left(\frac{v - V_3}{V_4}\right)}\right)
    \end{align}
    Furthermore, we have
    \begin{equation}
        m_\infty(v) = \frac{1}{2}\left(1 + \tanh{\left(\frac{v - V_1}{V_2}\right)}\right)
    \end{equation}

    \subsection{Morris-Lecar Parameters}

    % Parameters table
    The parameters from \cite{snm} used in this project are given in \reftab{tab:ml-param}. Sometimes $I$ and/or $phi$ are varied to assess the bifurcational behaviuor of the system.
    \begin{table}
    
        \centering
    
        \begin{adjustbox}{max width=\textwidth}
    
            \begin{tabular} { | c | c | c | }
                \hline
                Variable & Value & Units \\
                \hline\hline
                $C$ & 20 & $\mu F/cm^2$ \\
                \hline
                $g_L$ & 2.0 & $mS/cm^2$ \\
                \hline
                $\bar{g}_{Ca}$ & 4.4 & $mS/cm^2$ \\
                \hline
                $\bar{g}_K$ & 8 & $mS/cm^2$ \\
                \hline
                $V_L$ & -60 & $mV$ \\
                \hline
                $V_{Ca}$ & 120 & $mV$ \\
                \hline
                $V_K$ & -84 & $mV$ \\
                \hline
                $V_1$ & -1.2 & $mV$ \\
                \hline
                $V_2$ & 18.0 & $mV$ \\
                \hline
                $V_3$ & 2.0 & $mV$ \\
                \hline
                $V_4$ & 30.0 & $mV$ \\
                \hline
                $\phi$ & 0.04 & $ms^{-1}$ \\
                \hline
                $I$ & 90 & $\mu A/cm^2$ \\
                \hline
            \end{tabular}
    
        \end{adjustbox}
    
        \caption{TODO}
        
        \label{tab:ml-param}
    
    \end{table}

    \subsection{Helpful Resources}
    TODO

    \section{Dynamics}

    \subsection{Bautin Bifurcation}

    % "Canonical" equation
    Consider a 2-dimensional dynamical system $(x(t), y(t)) \in \mathbb{R}^2$. We may equivalently define the complex variable $z = x + iy \in \mathbb{C}$. Alternatively, we may express this system in polar coordinates: $z = re^{i\varphi}$. Now, let us consider the system
    \begin{align}
        \deriv{z}{t} &= \lambda z\left(1 + k_1\abs{z}^2 + k_2\abs{z}^4 + O\left(\abs{z}^5\right)\right) \\
        \label{eqn:bautin-canon}
        &= \lambda z + c_1 z\abs{z}^2 + c_2 z\abs{z}^4 + O\left(\abs{z}^6\right)
    \end{align}
    Here $c_j = \lambda k_j$, and $\lambda, c_j \in \mathbb{C}$ for $j = 1, 2$. Let us rewrite $\lambda = \alpha + i\omega, c_j = l_j + im_j$, where $\alpha, \omega, l_j, m_j \in \mathbb{R}$. Then taking real and imaginary parts of (\ref{eqn:bautin-canon}) gets us
    \begin{align}
        \deriv{x}{t} &= (\alpha + l_1 r^2 + l_2 r^4)x - (\omega + m_1 r^2 + m_2 r^4)y \\
        \deriv{y}{t} &= (\omega + m_1 r^2 + m_2 r^4)x + (\alpha + l_1 r^2 + l_2 r^4)y
    \end{align}
    Here we dropped terms of order $O\left(\abs{z}^6\right)$ or higher. We do so by assuming that $l_2 < 0$ (and is thus nonzero). The parameter $l_2$, known as the \textit{second Lyapunov coefficient}, is indeed negative for the Morris-Lecar neuron (as we shall discuss later). Doing some calculus, the equations for $r$ and $\varphi$ become:
    \begin{align}
        \deriv{r}{t} &= r(\alpha + l_1 r^2 + l_2 r^4) \\
        \deriv{\varphi}{t} &= \omega + m_1 r^2 + m_2 r^4
    \end{align}

    % Fixed points + limit cycles
    We see that there is a fixed point at $r = 0$. Furthermore, there are circular limit cycles at $r^2$ satisfying the quadratic
    \begin{equation}
        \label{eqn:bautin-quad}
        f(r^2) = r^4 + \frac{l_1}{l_2} r^2 + \frac{\alpha}{l_2} = 0
    \end{equation}
    Since these limit cycles occur at $r > 0$, we have precisely one limit cycle for each $r^2$ root. The discriminant $\Delta$ of the quadratic (\ref{eqn:bautin-quad}) is
    \begin{equation}
        \Delta = \left(\frac{l_1}{l_2}\right)^2 - 4\frac{\alpha}{l_2} = \frac{1}{l_2^2} \left(l_1^2 - 4\alpha l_2\right)
    \end{equation}
    The sign of $\Delta$ is therefore just the sign of $l_1^2 - 4\alpha l_2$. Then a necessary condition for there to be limit cycles i.e. positive real roots for $r^2$ is:
    \begin{equation}
        \alpha \geq \frac{l_1^2}{4l_2}
    \end{equation}

    % Delta >= 0
    Now assuming that $\Delta \geq 0$ i.e. the roots for $r^2$ are all real, we use Vieta's formulas to note that the sum and product of the roots are $-\frac{l_1}{l_2}, \frac{\alpha}{l_2}$, repsectively.
    
    % alpha > 0
    For $\Delta \geq 0$, exactly one positive root exists precisely when $\alpha > 0 \therefore \frac{\alpha}{l_2} < 0$. But we immediately have $\alpha > 0 \geq \frac{l_1}{4l_2}$, satisfying the discriminant condition.

    % alpha = 0
    Now if $\alpha = 0$, then at least one root must coincide with $r = 0$. The other root is thus equal to $-\frac{l_1}{l_2}$ and is positive iff $l_1 > 0$.

    % alpha < 0
    Next, if $\alpha < 0$, both roots have the same sign. If $l_1 \leq 0$ then the sum of the roots is nonpositive, resulting in both roots being nonpositive. Thus there are no limit cycles for $\alpha < 0, l_1 \leq 0$. However, if $\alpha < 0$ and $l_1 > 0$, there are two positive roots for $r^2$ if $\alpha > \frac{l_1^2}{4l_2}$. If instead, $l_1 > 0$ and $\alpha = \frac{l_1^2}{4l_2} < 0$ then there is a repeated positive root and therefore one limit cycle.

    % Stability
    Finally, the stability of the limit cycles and $r = 0$ fixed point is determined by the sign of $f(r^2)$ in between these roots. Since $l_2 < 0$, the outermost limit cycle (or fixed point if none exists) attracts from $r = \infty$. Meanwhile, the stability of the fixed point is determined by the sign of $\alpha$ (or $l_1$ if $\alpha = 0$, or $l_2 < 0$ if $\alpha, l_1 = 0$). Finally, if there are two limit cycles, the inner one is unstable while the outer one is stable since $f(r^2)$ is a downwards parabola in $r^2$.

    % Bifurcation diagram
    Put all together, the bifurcation diagram is given in \reffig{fig:bautin}
    \begin{figure}

        \centering
    
        \centerfig{0.65}{bautin.jpg}
        \caption{TODO + cite Kunetzsov}
    
        \label{fig:bautin}
    
    \end{figure}

    % Hopf bifurcation
    The curve $H_- = \{\alpha = 0, l_1 < 0\}$, in \reffig{fig:bautin} is a \textit{Hopf bifurcation}. For a fixed $l_1 < 0$, varying $\alpha$ near zero affects the stability of the system; $\alpha < 0$ has a limit cycle whereas $\alpha > 0$ does not. Similarly, $H_+ = \{\alpha = 0, l_1 > 0\}$ is also a Hopf bifurcation. This time, a limit cycle is produced at the fixed point when varying $\alpha > 0$ to $\alpha < 0$. In general, the Hopf bifurcation is a local codim-1 bifurcation, where a limit cycle appears from/disappears into a fixed point.

    % SNLC bifurcation
    The curve $T = \{l_1 > 0, \alpha = \frac{l_1^2}{4l_2}\}$ is a \textit{saddle node of limit cycles (SNLC) bifurcation}. Here the two limit cycles in region 3 collapse into a single limit cycle on $T$. Varying a parameter more (e.g. decreasing $\alpha$ or $l_1$) will cause the semi-stable limit cycle to disappear. Like the Hopf bifurcation, the SNLC bifurcation is a local codim-1 bifurcation.

    % Bautin bifurcation
    Finally, a \textit{Bautin bifurcation} occurs near the point $\alpha, l_1 = 0$ in this model system. Since two parameters are varied in this bifurcation ($\alpha, l_1$), this is a local codim-2 bifurcation.

    \subsection{Morris-Lecar Dynamics}

    \subsection{Scaling Results}

    \subsection{Helpful Resources}

    \section{Stochastics}

    \subsection{It\^o Calculus}

    % Wiener process TODO: cite finbook
    $X_t$ is a \textit{Wiener process} iff it is a continuous-time stochastic process satisfying the following conditions:
    \begin{enumerate}
        \item \textbf{Stationary increments:} If $s \leq t$, $X_t - X_s$ has the same distribution as $X_{t-s} - X_0$.
        \item \textbf{Independent increments:} If $s \leq t$, $X_t - X_s$ is independent of $X_r$ for all $r \leq s$.
        \item \textbf{Continuous paths:} $X_t$ is a continuous function of $t$.
    \end{enumerate}

    Here, a Wiener process is characterized by three paramters: the initial condition $X_0$, the drift $m$, and the variance parameter $\sigma^2$. For $s \leq t$, the increment $X_t - X_s$ is distributed as a Gaussian with mean $m(t - s)$ and variance $\sigma^2(t-s)$:
    \begin{equation}
        X_t - X_s \sim N\left(m(t - s), \sigma^2 (t - s)\right)
    \end{equation}
    The \textit{standard} Wiener process $B_t$ has $B_0 = 0$, $m = 0$, and $\sigma^2 = 1$. We may therefore write
    \begin{equation}
        dX_t = m dt + \sigma dB_t
    \end{equation}
    Note that $dB_t$ is distributed as
    \begin{equation}
        dB_t \sim \sqrt{dt}Z
    \end{equation}
    where $Z$ is a standard normal random variable. Now in general, if $f$ is a function of $X_t$, then we may write
    \begin{equation}
        df(X_t) = g(X_t) dt + h(X_t) dB_t
    \end{equation}
    for some functions $g(X_t), h(X_t)$ (It\^o's lemma gives a more precise result, but we will not utilize it for these purposes).

    The $n$-dimensional standard Wiener process $\mathbf{B_t}$ is simply a vector of $n$ independent standard Wiener processes. Then an $n$-dimensional dyanmical system $\mathbf{X}$ can be expressed in the form
    \begin{equation}
        \label{eqn:sde-general}
        d\mathbf{X} = \mathbf{g}(\mathbf{X}) dt + \mathbf{H}(\mathbf{X}) \mathbf{dB_t}
    \end{equation}
    Here $\mathbf{g}$ is a vector while $\mathbf{H}$ is a matrix.

    \subsection{Stochastic Morris-Lecar}

    % Stochastic model
    We can model the neuron as having $N_K$ potassium channels (in this project, we are typically on the order of $N_K \sim 1000$), each of which can be fully open or closed. The opening rate (i.e. probability rate of transition from the closed state to the open state) and closing rate for each channel would be $\alpha(v)$ and $\beta(v)$ from (\ref{eqn:w-dyn}), respectively. Then over a small time increment $dt$, the probability of a closed channel opening is $\alpha(v) dt$. Likewise, the probability of an open channel closing is $\beta(v) dt$.

    % Apply CLT
    Over a time increment of $dt$, we have
    \begin{equation}
        \label{eqn:w-stochastic-bare}
        dw = w(t + dt) - w(t) \sim \frac{\Binom{N_K (1 - w)}{\alpha dt} - \Binom{N_K w}{\beta dt}}{N_K}
    \end{equation}
    Note that $\Binom{n, p} = \sum_{j = 1}^n I_j$, where each $I_j$ is an independent indicator random variable with probability $p$. By the central limit theorem, when $n$ is large
    \begin{equation}
        \Binom{n}{p} \sim np + \sqrt{np(1-p)} Z
    \end{equation}
    where $Z$ is a standard normal random variable. Then (\ref{eqn:w-stochastic-bare}) becomes
    \small
    \begin{equation}
        dw \sim \frac{\left(N_K (1 - w) \alpha dt + \sqrt{N_K(1 - w) \alpha dt (1 - \alpha dt)}Z_1\right) - \left(N_K w \beta dt + \sqrt{N_K w \beta dt (1 - \beta dt)}Z_2\right)}{N_K}
    \end{equation}
    \normalsize
    Here $Z_1$ and $Z_2$ are independent normal random variables. This is because the closed and open channels are independent (as we assumed all the channels to be independent). Since $dt$ is small, $1 - \alpha dt \approx 1$. So
    \begin{equation}
        dw \sim \left(\alpha (1 - w) - \beta w\right) dt + \frac{1}{\sqrt{N_K}}\sqrt{\alpha (1 - w) + \beta w}\sqrt{dt}Z
    \end{equation}
    As a system of stochastic differential equations, we have
    \begin{align}
        \label{eqn:ml-stochastic}
        \qvec{v}{w}' &= \qvec{f(v, w)}{g(v, w)} dt + \qmat{0}{0}{0}{h(v, w)} \mathbf{dB_t} \\
        f(v, w) &= \frac{I - \bar{g}_K w (v - V_K) - \bar{g}_{Ca} m_\infty(v) (v - V_{Ca}) - g_L (v - V_L)}{C} \\
        g(v, w) &= \alpha(v) (1 - w) - \beta(v) w \\
        \label{eqn:ml-stochastic-end}
        h(v, w) &= \frac{1}{\sqrt{N_K}}\sqrt{\alpha(v) (1 - w) + \beta(v) w}
    \end{align}

    \subsection{Euler-Maruyama Method}

    % General description
    The Euler-Maruyama method is a generalization of the Euler method to stochastic differential equations of the form (\ref{eqn:sde-general}). Here, a small finite time increment $dt$ is chosen. Then, starting from an initial condition $\mathbf{X}(0)$, we iteratively compute
    \begin{equation}
        \mathbf{X}(t + dt) = \mathbf{X}(t) + \mathbf{g}(\mathbf{X}) dt + \mathbf{H}(\mathbf{X}) \mathbf{Z} \sqrt{dt}
    \end{equation}
    Here $\mathbf{Z}$ is a randomly generated vector, where each entry is obtained from an independent unit normal distribution. The resulting path $\mathbf{X}(t)$ will be distributed approximately as the true distribution; thus, many trials of the Euler-Maruyama method can be used to obtain statistics for the process (e.g. interspike intervals, which will be discussed later).

    % ML-specific
    To ease computations, only the second entry of $\mathbf{Z}$ needs to be generated for the 2-dimensional stochastic Morris-Lecar model (since only $\mathbf{H_{22}}$ is nonzero). Furthermore, the random numbers can all be generated at the start rather than at every iteration.

    \subsection{Helpful Resources}
    % TODO

    \section{Interspike Intervals}

    \subsection{Defining Intervals}

    \subsection{Results}

    \section{Poincare-Like Maps} % TODO change name?
    % TODO

    \section{Alternative Dynamics}

    \subsection{Jacobi Dynamics}

    % Motivation
    Physically, $w$ should be bounded in $[0, 1]$. However, the equations described in (\ref{eqn:ml-stochastic})-(\ref{eqn:ml-stochastic-end}) imply a finite (albeit small) possibility of $w$ being outside of $[0, 1]$. \textit{In practice}, this issue never really manifests. For theoretical satisfaction, however, we may instead use--for a fixed $v$--a bounded diffusion process for $w$.

    % Jacobi diffusion
    Following \cite{dg}, we choose the simplest diffusion process that mimics the system. Arguably, we wish to preserve the dynamic part $g(v, w)$ while having as simple of a variance coefficient $h(v, w)^2$ as possible. The preservation of $g(v, w)$ can be vouched for as it, loosely speaking, determines the ``average'' behaviour of the system, as well as defining the $N_K \to 0$ limit behaviour. Seeing that $g(v, w)$ is linear in $w$, we consider Pearson diffusions, where $h(v, w)^2$ is a polynomial of degree at most two. It turns out that the Jacobi diffusion is the only Pearson diffusion bounded in a finite interval. This renders a Jacobi diffusion as the ``simplest'' bounded diffusion process appropriate for the stochastic Morris-Lecar system.

    % Ergodicity
    We follow the discussion \cite{dg} on the Jacobi diffusion here. The Jacobi diffusion bounded on $(0, 1)$ has the form (for fixed $v$)
    \begin{equation}
        \label{eqn:w-jacobi-base}
        dw = -\theta (w - \mu) dt + \gamma\sqrt{2\theta w (1 - w)} dB_t
    \end{equation}
    Equating the dynamic part gets us
    \begin{equation}
        \theta = \alpha + \beta, \quad \mu = \frac{\alpha}{\alpha + \beta}
    \end{equation}
    Note that we necessarily have $\alpha(v), \beta(v) \geq 0$ as can be seen from (\ref{eqn:alpha-def})-(\ref{eqn:beta-def}). The Jacobi diffusion is \textit{ergodic} for
    \begin{equation}
        \label{eqn:jacobi-ergodic-condition}
        \gamma^2 \leq \mu, 1 - \mu
    \end{equation}
    To retain this property, we can rewrite $\gamma = \sigma^* \delta(v)$, where $\sigma^*$ is a constant that can be fit (to e.g. match $h(v, w)$ at the fixed point) and $\delta(v)$ is a simple function of $v$. Of course, such a choice of $\sigma^*, \delta(v)$ is not unique since we can rewrite $\sigma^* \mapsto x\sigma^*$, $\delta(v) \mapsto \frac{\delta(v)}{x}$ for any nonzero $x$. We therefore assert the condition that $\sigma^* \in [0, 1]$ and that (\ref{eqn:jacobi-ergodic-condition}) holds for all $\sigma^*$ in that range. When fitting $\sigma^*$, the ergodicity requirement becomes verifying that $\sigma^* \in [0, 1]$.

    % Jacobi parameters
    $\gamma^2$ is largest when $\sigma^* = 1$, so our task becomes assuring that $\delta(v)^2 \leq \mu, 1 - \mu$ i.e.
    \begin{equation}
        \delta(v)^2 \leq \frac{\alpha(v)}{\alpha(v) + \beta(v)}, \frac{\beta(v)}{\alpha(v) + \beta(v)}
    \end{equation}
    Naively, we could define
    \begin{equation}
        \delta(v) = \sqrt{\min{\left(\frac{\alpha(v)}{\alpha(v) + \beta(v)}, \frac{\beta(v)}{\alpha(v) + \beta(v)}\right)}}
    \end{equation}
    However, the piecewise definition comes at the cost of differentiability. To make $\delta(v)$ analytically ``well-behaved'', we instead define $\delta(v)^2$ to be the harmonic sum of $\mu, 1 - \mu$ (as is done for e.g. parallel resistors):
    \begin{equation}
        \delta(v)^2 = \frac{\mu(1 - \mu)}{\mu + (1 - \mu)} = \frac{\alpha\beta}{\alpha + \beta}
    \end{equation}
    Thus
    \begin{equation}
        \gamma = \sigma^* \sqrt{\frac{\alpha\beta}{\alpha + \beta}}
    \end{equation}
    Equation (\ref{eqn:w-jacobi-base}) now becomes
    \begin{equation}
        \label{eqn:w-jacobi}
        dw = (\alpha(v) (1 - w) - \beta(v) w) dt + \sigma^* \sqrt{2\frac{\alpha(v)\beta(v)}{\alpha(v) + \beta(v)} w(1-w)} dB_t
    \end{equation}

    % sigma* fitting
    The fixed point of the system occurs at approximately $(v_{eq}, w_{eq}) = (-26.6, 0.129)$. Plugging this into (\ref{eqn:ml-stochastic-end}) and the coefficient in front of $dB_t$ in (\ref{eqn:w-jacobi}) gives
    \begin{equation}
        \label{eqn:h-eq}
        h(v_{eq}, w_{eq}) = \frac{0.1003}{\sqrt{N_K}} = 0.03365\sigma^*
    \end{equation}
    That gives us
    \begin{equation}
        \sigma^* = \frac{2.98}{\sqrt{N_K}}
    \end{equation}
    Since $N_K$ is on the order of $1000$, and certainly larger than $9$, we see that typical fits of $\sigma^*$ are indeed ergodic.

    \subsection{Linearized Dynamics}
    
    % Linear equation
    Near the fixed point, we can expand (\ref{eqn:ml-stochastic}) as a linear model as in \cite{dg}. Since the deterministic part is zero to zeroth order, we expand it to first order. In contrast, the stochastic part $h(v, w)$ is nonzero to zeroth order, so we need only expand that to zeroth order. That gets us
    \begin{equation}
        \qvec{v}{w}' \approx \mathbf{M} \qvec{v}{w} dt + \qvec{0}{\frac{0.1003}{\sqrt{N_K}} dB_t}
    \end{equation}
    Here we used the value of (\ref{eqn:h-eq}) for the stochastic part. Meanwhile, $\mathbf{M}$ is the Jacobian matrix of the dynamic part in (\ref{eqn:ml-stochastic}):
    \begin{equation}
        \mathbf{M} = D\qvec{f(v, w)}{g(v, w)}
    \end{equation}

    % X~ transformation
    The linear approximation is valid in a small region near the fixed point. Of course, this region is necessarily within the unstable limit cycle. Numerical experiemnts can provide more clarity into the size of this region. Such a region would be elliptical, however. This is because the coordinates $v, w$ are not symmetrical in the sense that $\mathbf{M}$ does not stretch and shear them uniformly. To remove this complication and simplify the equation, we can define the matrix
    \begin{equation}
        \mathbf{Q} = \qmat{\omega}{m_{11} + \lambda}{0}{m_{21}}
    \end{equation}
    Then we define
    \begin{align}
        \mathbf{A} &= \mathbf{Q}^{-1}\mathbf{M}\mathbf{Q} = \qmat{-\lambda}{\omega}{-\omega}{-\lambda} \\
        \tilde{\mathbf{x}} &= \mathbf{Q}^{-1}\qvec{v}{w} \\
        \mathbf{c} &= \mathbf{Q}^{-1} \qvec{0}{\frac{0.1003}{\sqrt{N_K}} dB_t}
    \end{align}
    We then obtain the symmetrized equation
    \begin{equation}
        d\tilde{\mathbf{x}} = \mathbf{A}\tilde{\mathbf{x}} + \mathbf{c}
    \end{equation}
    In the transformed space $\tilde{\mathbf{x}}$, it makes sense to talk about circular regions as opposed to elliptical regions in the original coordinate system. Finally, we can further simplify this equation by factoring out rotations. If we define
    \begin{equation}
        \tilde{\tilde{\mathbf{x}}} = R_{\omega t} \tilde{\mathbf{x}} = \qmat{\cos\omega t}{-\sin\omega t}{\sin\omega t}{\cos\omega t} \tilde{\mathbf{x}}
    \end{equation}
    Then we obtain the equation
    \begin{equation}
        d\tilde{\tilde{\mathbf{x}}} = -\lambda\tilde{\tilde{\mathbf{x}}} + R_{\omega t}\mathbf{c}
    \end{equation}

    \subsection{Numerical Comparison}
    % TODO

    \subsection{Helpful Resources}
    % TODO

    \section{Patched Model}
    % TODO

    \section{Next Steps}
    % TODO

    \pagebreak

    \bibliographystyle{plain}
    \bibliography{bibliography.bib}

\end{flushleft}
\end{document}
